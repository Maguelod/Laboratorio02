# -*- coding: utf-8 -*-
"""Laboratorio02

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K_uCoChmmKtbMOXTjSJDEp3dlGHJLbGm

REGRESION MULTIVARIABLE
"""

# Commented out IPython magic to ensure Python compatibility.
#librerias
import os
import numpy as np
from matplotlib import pyplot
# %matplotlib inline
import pandas as pd

# Cargando el dataset
from google.colab import drive
drive.mount('/content/drive')
data = pd.read_csv('/content/drive/MyDrive/train.csv')
data = data.dropna()
data = data.iloc[:15100, :20]

data.info()

# Seleccionamos las columnas independientes (X) y dependiente(y)
X = data.iloc[:15000, 3:13]
y = data.iloc[:15000, 2]
m = y.size
X_prediccion = data.iloc[15000:15100, 3:13]
y_prediccion = data.iloc[15000:15100, 2]
m_prediccion = y_prediccion.size

print(X.head())

display(data)

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])
    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

X_norm, mu, sigma = featureNormalize(X)
print('Media calculada:\n', mu)
print('Desviación estandar calculada:\n', sigma)
print(X_norm)

X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)
print (X[1])

def computeCostMulti(X, y, theta):
    m = y.shape[0]
    J = 0
    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))
    return J

def gradientDescentMulti(X, y, theta, alpha, num_iters):
    m = y.shape[0]
    theta = theta.copy()
    J_history = []
    for i in range(num_iters):
        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)
        J_history.append(computeCostMulti(X, y, theta))
    return theta, J_history

alpha = 0.001
num_iters = 5000
theta = np.zeros(11)
theta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)
print(J_history[-1])
pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')
pyplot.title('Convergencia del Descenso por el Gradiente')
pyplot.show()

print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta)))
X_array = [1,-6.7863,11.9081,5.0930,11.4607,-9.2834,5.1187,18.6266,-4.9200,5.7470,2.9252]
X_array[1:] = (X_array[1:] - mu) / sigma
Precio = np.dot(X_array, theta)

print('Prueba de prediccion: ${:.0f}'.format(Precio))

X_norm, mu, sigma = featureNormalize(X_prediccion)
X_regresion = np.concatenate([np.ones((m_prediccion, 1)), X_norm], axis=1)
for i in range(m_prediccion):
  X_array = np.array(X_regresion[i])
  Prediccion = np.dot(X_array, theta)
  print('prediccion: ',Prediccion, 'valor real: ',y_prediccion.iloc[i])

"""ECUACION DE LA NORMAL"""

# Cargar datos
from google.colab import drive
drive.mount('/content/drive')
data = pd.read_csv('/content/drive/MyDrive/train.csv')
data = data.dropna()
data = data.iloc[:15100, :20]

display(data)

X = data.iloc[:15000, 3:13]
y = data.iloc[:15000, 2]
m = y.size
X_prediccion = data.iloc[15000:15100, 3:13]
y_prediccion = data.iloc[15000:15100, 2]
m_prediccion = y_prediccion.size

print(X.head())

X = np.concatenate([np.ones((m, 1)), X], axis=1)

# Funcion de la ecuacion de la normal, obtenemos valores optimos
def normalEqn(X, y):
    theta = np.zeros(X.shape[1])
    theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)
    return theta

# Calcula los parametros con la ecuación de la normal
theta = normalEqn(X, y)

# Muestra los resultados obtenidos a partir de la aplicación de la ecuación de la normal
print('Theta calculado a partir de la ecuación de la normal: {:s}'.format(str(theta)))
X_array = [1,-6.7863,11.9081,5.0930,11.4607,-9.2834,5.1187,18.6266,-4.9200,5.7470,2.9252]
Ycalculada = np.dot(X_array, theta)
print('Prueba de prediccion: ',Ycalculada)
j=computeCostMulti(X, y, theta)
print('Costo: ',j)

X_regresion = np.concatenate([np.ones((m_prediccion, 1)), X_prediccion], axis=1)
for i in range(m_prediccion):
  X_array = np.array(X_regresion[i])
  Prediccion = np.dot(X_array, theta)
  print('prediccion: ',Prediccion, 'valor real: ',y_prediccion.iloc[i])

"""REGRESION POLINOMIAL"""

# Cargar datos
from google.colab import drive
drive.mount('/content/drive')
data = pd.read_csv('/content/drive/MyDrive/train.csv')
data = data.dropna()
data = data.iloc[:15100, :20]

data.info()

X = data.iloc[:15000, 3:13]
y = data.iloc[:15000, 2]
m = y.size
X_prediccion = data.iloc[15000:15100, 3:13]
y_prediccion = data.iloc[15000:15100, 2]
m_prediccion = y_prediccion.size

print(X)
print(y)
print(m)

X = np.concatenate([X, X * X], axis=1)
print(X[1])

print(X)

num_filas, num_columnas = X.shape
print("Número de filas:", num_filas)
print("Número de columnas:", num_columnas)

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

# llama featureNormalize con los datos cargados
X_norm, mu, sigma = featureNormalize(X)

print(X)
print('Media calculada:', mu)
print('Desviación estandar calculada:', sigma)
print(X_norm)

X = np.concatenate([np.ones((m, 1)), X_norm], axis=1)
print (X[1])

print(X)

def computeCostMulti(X, y, theta):
    m = y.shape[0]

    J = 0

    h = np.dot(X, theta)

    J = (1/(2 * m)) * np.sum(np.square(np.dot(X, theta) - y))

    return J

def gradientDescentPoly(X, y, theta, alpha, num_iters):

    m = y.shape[0]

    theta = theta.copy()

    J_history = []

    for i in range(num_iters):
        theta = theta - (alpha / m) * (np.dot(X, theta) - y).dot(X)
        J_history.append(computeCostMulti(X, y, theta))

    return theta, J_history

num_filas, num_columnas = X.shape
print("Número de filas:", num_filas)
print("Número de columnas:", num_columnas)

alpha = 0.001
num_iters = 10000

theta = np.zeros(21)
theta, J_history = gradientDescentPoly(X, y, theta, alpha, num_iters)
print(J_history[-1])
pyplot.plot(np.arange(len(J_history)), J_history, lw=2)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')

print('theta calculado por el descenso por el gradiente: ',str(theta))

X_array = [1,-6.7863,11.9081,5.0930,11.4607,-9.2834,5.1187,18.6266,-4.9200,5.7470,2.9252,
           -6.7863*-6.7863,11.9081*11.9081,5.0930*5.0930,11.4607*11.4607,-9.2834*-9.2834,5.1187*5.1187
           ,18.6266*18.6266,-4.9200*-4.9200,5.7470*5.7470,2.9252*2.9252]
X_array[1:] = (X_array[1:] - mu) / sigma
Ycalculada = np.dot(X_array, theta)

print('Y Predicha: ',Ycalculada)

X_prediccion= np.concatenate([X_prediccion, X_prediccion * X_prediccion], axis=1)
X_norm, mu, sigma = featureNormalize(X_prediccion)
X_regresion = np.concatenate([np.ones((m_prediccion, 1)), X_norm], axis=1)
for i in range(m_prediccion):
  X_array = np.array(X_regresion[i])
  Prediccion = np.dot(X_array, theta)
  print('prediccion: ',Prediccion, 'valor real: ',y_prediccion.iloc[i])